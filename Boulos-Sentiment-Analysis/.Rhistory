library(tidytext)
source('C:/Users/victor/Desktop/LearnR/Sentimento/Presidentes-Americanos.R', encoding = 'UTF-8', echo=TRUE)
source('C:/Users/victor/Desktop/LearnR/Sentimento/Sentiment-Analysis/Boulos-Sentiment-Analysis/Boulos 2.R', encoding = 'UTF-8', echo=TRUE)
tweets.boulos <- searchTwitter("Boulos", n = 10000, lang = "pt-br")
text <- as.character(rep(NA, length(tweets.boulos)))
screenname <- as.character(rep(NA, length(tweets.boulos)))
created = as.POSIXct(rep(NA, length(tweets.boulos)))
id = c()
for (i in 1:length(tweets.boulos)) {
text[i] = tweets.boulos[[i]]$text
screenname[i] = tweets.boulos[[i]]$screenName
created[i] = tweets.boulos[[i]]$created
id[i] = tweets.boulos[[i]]$id
}
x = data.frame(id = id,
screenname = screenname,
created = created,
text = text,
stringsAsFactors = FALSE)
View(x)
x$text <- str_replace_all(x$text,"\n", " ")
bing = read.delim("c:/Users/victor/Desktop/LearnR/Sentimento/Lexicos/oplexicon_v3.0/lexico_v3.0.txt",
header = FALSE,
sep = ",")
bing$V2 <- NULL
bing$V4 <- NULL
names(bing) <- c("word", "sentiment")
bing$sentiment <- ifelse(bing$sentiment == 1, "positive",
ifelse(bing$sentiment == 0, "neutral",
"negative"))
stopwords <- tm::stopwords("pt-br")
x %>%
unnest_tokens(token = "words", word, text) %>%
select(id, word) %>%
filter(!word %in% stopwords, !word == "rt") %>%
inner_join(bing, by = c("word" = "word")) %>%
count(word, sentiment, sort = TRUE) %>%
dplyr_row_slice(1:50)
x %>%
unnest_tokens(token = "words", word, text) %>%
select(id, word) %>%
filter(!word %in% stopwords, !word == "rt") %>%
inner_join(bing, by = c("word" = "word")) %>%
count(word, sentiment, sort = TRUE) %>%
group_by(sentiment) %>%
summarise(n = sum(n))
x %>%
unnest_tokens(token = "words", word, text) %>%
filter(!word %in% stopwords, !word == "rt") %>%
select(id, created, word) %>%
inner_join(bing, by = c("word" = "word")) %>%
select(id, created, sentiment) %>%
count(id, created, sentiment) %>%
filter(sentiment == "positive" | sentiment == "negative") %>%
group_by(Date = as.Date(ymd_hms(created)), sentiment) %>%
summarize(total = sum(n)) %>%
ggplot(aes(x = sentiment, y = total, fill = sentiment)) +
geom_bar(stat = "identity", show.legend = FALSE) +
labs(x = "Sentimento",
y = "Tweets",
title = "Análise de Sentimento - Boulos 28/11/2020") +
scale_x_discrete(position = "bottom",
labels =  c("negative" = "Negativo",
"positive" = "Positivo"))
x %>%
unnest_tokens(token = "words", word, text) %>%
filter(!word %in% stopwords, !word == "rt") %>%
select(id, screenname, created, word) %>%
inner_join(bing, by = c("word" = "word")) %>%
count(screenname, sentiment, sort = TRUE) %>%
filter(sentiment != "neutral") %>%
group_by(sentiment) %>%
top_n(10) %>%
ggplot(aes(x=factor(screenname), y = n, fill= sentiment))
+ geom_bar(stat='identity') +
theme(axis.text.x = element_text(angle=90)) +
labs(x = '', y = '',
title = "Análise de Sentimento Boulos - 28/11/2020",
subtitle = "por usuário",
fill = "Sentimento") +
scale_fill_discrete(labels = c("negative" = "Negativo",
"positive" = "Positive"))
# Setando a Key da API
setup_twitter_oauth(consumer_key = "Hv9IlPQdXanLQA3RmKwYj5GbA",
consumer_secret = "fM62rOf1yt4dRso9DLgA0WqUiBdVlDK6N78tc8la0PdezqwtqG",
access_token = "1321975861675962369-zjg4qTfCjzlBwq3BXMiogr27mShozI",
access_secret = "aeYkc0t6vK68mQMP70TnBSHXE5CGLi7qRn8oVVh1A5EGU")
tweets.boulos <- searchTwitter("Boulos", n = 10000, lang = "pt-br")
text <- as.character(rep(NA, length(tweets.boulos)))
screenname <- as.character(rep(NA, length(tweets.boulos)))
created = as.POSIXct(rep(NA, length(tweets.boulos)))
id = c()
for (i in 1:length(tweets.boulos)) {
text[i] = tweets.boulos[[i]]$text
screenname[i] = tweets.boulos[[i]]$screenName
created[i] = tweets.boulos[[i]]$created
id[i] = tweets.boulos[[i]]$id
}
x = data.frame(id = id,
screenname = screenname,
created = created,
text = text,
stringsAsFactors = FALSE)
View(x)
x$text <- str_replace_all(x$text,"\n", " ")
bing = read.delim("c:/Users/victor/Desktop/LearnR/Sentimento/Lexicos/oplexicon_v3.0/lexico_v3.0.txt",
header = FALSE,
sep = ",")
bing$V2 <- NULL
bing$V4 <- NULL
names(bing) <- c("word", "sentiment")
bing$sentiment <- ifelse(bing$sentiment == 1, "positive",
ifelse(bing$sentiment == 0, "neutral",
"negative"))
stopwords <- tm::stopwords("pt-br")
x %>%
unnest_tokens(token = "words", word, text) %>%
select(id, word) %>%
filter(!word %in% stopwords, !word == "rt") %>%
inner_join(bing, by = c("word" = "word")) %>%
count(word, sentiment, sort = TRUE) %>%
dplyr_row_slice(1:50)
x %>%
unnest_tokens(token = "words", word, text) %>%
select(id, word) %>%
filter(!word %in% stopwords, !word == "rt") %>%
inner_join(bing, by = c("word" = "word")) %>%
count(word, sentiment, sort = TRUE) %>%
group_by(sentiment) %>%
summarise(n = sum(n))
x %>%
unnest_tokens(token = "words", word, text) %>%
filter(!word %in% stopwords, !word == "rt") %>%
select(id, created, word) %>%
inner_join(bing, by = c("word" = "word")) %>%
select(id, created, sentiment) %>%
count(id, created, sentiment) %>%
filter(sentiment == "positive" | sentiment == "negative") %>%
group_by(Date = as.Date(ymd_hms(created)), sentiment) %>%
summarize(total = sum(n)) %>%
ggplot(aes(x = sentiment, y = total, fill = sentiment)) +
geom_bar(stat = "identity", show.legend = FALSE) +
labs(x = "Sentimento",
y = "Tweets",
title = "Análise de Sentimento - Boulos 28/11/2020") +
scale_x_discrete(position = "bottom",
labels =  c("negative" = "Negativo",
"positive" = "Positivo"))
x %>%
unnest_tokens(token = "words", word, text) %>%
filter(!word %in% stopwords, !word == "rt") %>%
select(id, screenname, created, word) %>%
inner_join(bing, by = c("word" = "word")) %>%
count(screenname, sentiment, sort = TRUE) %>%
filter(sentiment != "neutral") %>%
group_by(sentiment) %>%
top_n(10) %>%
ggplot(aes(x=factor(screenname), y = n, fill= sentiment))
+ geom_bar(stat='identity') +
theme(axis.text.x = element_text(angle=90)) +
labs(x = '', y = '',
title = "Análise de Sentimento Boulos - 28/11/2020",
subtitle = "por usuário",
fill = "Sentimento") +
scale_fill_discrete(labels = c("negative" = "Negativo",
"positive" = "Positive"))
x %>%
unnest_tokens(token = "words", word, text) %>%
filter(!word %in% stopwords, !word == "rt") %>%
select(id, screenname, created, word) %>%
inner_join(bing, by = c("word" = "word")) %>%
count(screenname, sentiment, sort = TRUE) %>%
filter(sentiment != "neutral") %>%
group_by(sentiment) %>%
top_n(10) %>%
ggplot(aes(x=factor(screenname), y = n, fill= sentiment))
+ geom_bar(stat='identity') +
theme(axis.text.x = element_text(angle=90)) +
labs(x = '', y = '',
title = "Análise de Sentimento Boulos - 28/11/2020",
subtitle = "por usuário",
fill = "Sentimento") +
scale_fill_discrete(labels = c("negative" = "Negativo",
"positive" = "Positive"))
require(twitteR)
require(tidyverse)
require(httr)
require(knitr)
require(rmarkdown)
require(tidytext)
require(stringr)
require(ggplot2)
require(RColorBrewer)
require(stringi)
require(tm)
fullwords <- read.delim("lexico_v3.0.txt", ",", header = FALSE)
head(fullwords, 2)
names(fullwords) <- c("lexico", "type", "sentiment", "mode")
## Filter data
positivewords <- fullwords %>% filter(sentiment == 1)
head(positivewords, 2)
negativewords <- fullwords %>% filter(sentiment == -1)
head(negativewords, 2)
## Finding tweets
setup_twitter_oauth(consumer_key = "",
consumer_secret = "",
access_token = "",
access_secret = "")
boulos <- searchTwitter("Boulos", n = 10000, lang = "pt")
txt.boulos <- sapply(boulos, function(x) x$getText())
txt.boulos <- iconv(txt.boulos, to = "UTF-8", sub = "")
## Cleaning Tweets
# Function to clear
clear.tweets <- function(tweets){
# remove retweet entities
tweets <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets)
# remove at people
tweets <- gsub("@\\w+", "", tweets)
# remove punctuation
tweets <- gsub("[[:punct:]]", "", tweets)
# remove numbers
tweets <- gsub("[[:digit:]]", "", tweets)
# remove html links
tweets <- gsub("(f|https?)(://)(.*)", "", tweets)
tweets <- gsub("http\\w+", "", tweets)
# remove unnecessary spaces
tweets <- gsub("[ \t]{2,}", "", tweets)
tweets <- gsub("^\\s+|\\s+$", "", tweets)
# changing encoding and tolower
tweets <- stri_trans_general(tweets, "latin-ascii")
tryTolower <- function(x){
y = NA
try_error = tryCatch(tolower(x), error = function(e) e)
if(!inherits(try_error, "error")){
y <- tolower(x)}
return(y)
}
tweets <- tryTolower(tweets)
tweets <- iconv(tweets, from = "UTF-8", to = "ASCII")
}
txt.boulos <- clear.tweets(txt.boulos)
head(txt.boulos, 3)
## Removing stopwords and NA's
txt.boulos <- removeWords(txt.boulos, stopwords("portuguese"))
head(txt.boulos, 5)
txt.boulos <- txt.boulos[!is.na(txt.boulos)]
head(txt.boulos, 5)
## Geting Score: Positive or Negative
scores = lapply(txt.boulos, function(tweets, positive.words = positivewords$lexico, negative.words = negativewords$lexico){
listtweets <- str_split(tweets, "\\s+")
tweets <- unlist(listtweets)
find.positives <- match(tweets, positive.words)
find.negatives <- match(tweets, negative.words)
positive.score <- sum(!is.na(find.positives))
negative.score <- sum(!is.na(find.negatives))
score = positive.score - negative.score
return(c(score, positive.score, negative.score))
})
# Getting score
tot.score <- vector()
length(tot.score) <- length(scores)
for (i in 1:length(scores)) {
total = scores[[i]][1]
tot.score[i] <- total
}
rm(i)
# Gettting positive score
pos.score <- vector()
length(pos.score) <- length(scores)
for (i in 1:length(scores)) {
pos = scores[[i]][2]
pos.score[i] <- pos
}
rm(i)
# Getting negative score
neg.score <- vector()
length(neg.score) <- length(scores)
for (i in 1:length(scores)) {
neg = scores[[i]][3]
neg.score[i] <- neg
}
rm(i)
neg.score
df.scores <- data.frame(tweet = txt.boulos, score = tot.score, positive.score = pos.score, negative.score = neg.score)
df.scores$score <- ifelse(df.scores$score == 0, "Neutral", ifelse(df.scores$score > 0, "Positive", "Negative"))
df.scores$score <- as.factor(df.scores$score)
df.scores %>%
group_by(score) %>%
count() %>%
ggplot(aes(x = reorder(score, -n), y = n, fill = score)) +
geom_bar(stat = "identity") +
labs(title = "Análise de Sentimento - Tweets Relacionados ao Boulos",
subtitle = "Dados coletados do twitter em 27/11/2020",
fill = "Sentimento") +
xlab(NULL) +
ylab("Quantidade de Tweets") +
theme_tinyhand()
df.scores %>%
summarise(sumtot = c(sum(positive.score), sum(negative.score))) %>%
mutate(type = c("Positive", "Negative")) %>%
ggplot(aes(x = type, y = sumtot, fill = type)) +
geom_bar(stat = "identity", show.legend = FALSE) +
labs(title = "Análise de Sentimento - Guilherme Boulos",
subtitle = "Tweets coletados em 27/11/2020",
x = "Sentimento",
y = "Quantidade") +
ggplot2::theme_minimal()
tweetcorpus <- Corpus(VectorSource(txt.boulos))
pal2 <- brewer.pal(8, "Set2")
wordcloud(tweetcorpus,
min.freq = 2,
scale = c(5, 1),
random.color = FALSE,
max.words = 60,
random.order = F,
colors = pal2)
require(twitteR)
require(tidyverse)
require(httr)
require(knitr)
require(rmarkdown)
require(tidytext)
require(stringr)
require(ggplot2)
require(RColorBrewer)
require(stringi)
require(tm)
fullwords <- read.delim("lexico_v3.0.txt", ",", header = FALSE)
head(fullwords, 2)
names(fullwords) <- c("lexico", "type", "sentiment", "mode")
## Filter data
positivewords <- fullwords %>% filter(sentiment == 1)
head(positivewords, 2)
negativewords <- fullwords %>% filter(sentiment == -1)
head(negativewords, 2)
## Reading Positive/Negatives/Neutral words
fullwords <- read.delim("lexico_v3.0.txt", ",", header = FALSE)
## Reading Positive/Negatives/Neutral words
fullwords <- read.delim("C:/Users/victor/Desktop/LearnR/Sentimento/Lexicos/oplexicon_v3.0/lexico_v3.0.txt", ",", header = FALSE)
head(fullwords, 2)
names(fullwords) <- c("lexico", "type", "sentiment", "mode")
## Filter data
positivewords <- fullwords %>% filter(sentiment == 1)
head(positivewords, 2)
negativewords <- fullwords %>% filter(sentiment == -1)
head(negativewords, 2)
## Finding tweets
setup_twitter_oauth(consumer_key = "Hv9IlPQdXanLQA3RmKwYj5GbA",
consumer_secret = "fM62rOf1yt4dRso9DLgA0WqUiBdVlDK6N78tc8la0PdezqwtqG",
access_token = "1321975861675962369-zjg4qTfCjzlBwq3BXMiogr27mShozI",
access_secret = "aeYkc0t6vK68mQMP70TnBSHXE5CGLi7qRn8oVVh1A5EGU")
boulos <- searchTwitter("Boulos", n = 10000, lang = "pt")
txt.boulos <- sapply(boulos, function(x) x$getText())
txt.boulos <- iconv(txt.boulos, to = "UTF-8", sub = "")
clear.tweets <- function(tweets){
# remove retweet entities
tweets <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets)
# remove at people
tweets <- gsub("@\\w+", "", tweets)
# remove punctuation
tweets <- gsub("[[:punct:]]", "", tweets)
# remove numbers
tweets <- gsub("[[:digit:]]", "", tweets)
# remove html links
tweets <- gsub("(f|https?)(://)(.*)", "", tweets)
tweets <- gsub("http\\w+", "", tweets)
# remove unnecessary spaces
tweets <- gsub("[ \t]{2,}", "", tweets)
tweets <- gsub("^\\s+|\\s+$", "", tweets)
# changing encoding and tolower
tweets <- stri_trans_general(tweets, "latin-ascii")
tryTolower <- function(x){
y = NA
try_error = tryCatch(tolower(x), error = function(e) e)
if(!inherits(try_error, "error")){
y <- tolower(x)}
return(y)
}
tweets <- tryTolower(tweets)
tweets <- iconv(tweets, from = "UTF-8", to = "ASCII")
}
txt.boulos <- clear.tweets(txt.boulos)
head(txt.boulos, 3)
## Removing stopwords and NA's
txt.boulos <- removeWords(txt.boulos, stopwords("portuguese"))
head(txt.boulos, 5)
txt.boulos <- txt.boulos[!is.na(txt.boulos)]
head(txt.boulos, 5)
## Geting Score: Positive or Negative
scores = lapply(txt.boulos, function(tweets, positive.words = positivewords$lexico, negative.words = negativewords$lexico){
listtweets <- str_split(tweets, "\\s+")
tweets <- unlist(listtweets)
find.positives <- match(tweets, positive.words)
find.negatives <- match(tweets, negative.words)
positive.score <- sum(!is.na(find.positives))
negative.score <- sum(!is.na(find.negatives))
score = positive.score - negative.score
return(c(score, positive.score, negative.score))
})
# Getting score
tot.score <- vector()
length(tot.score) <- length(scores)
for (i in 1:length(scores)) {
total = scores[[i]][1]
tot.score[i] <- total
}
rm(i)
# Gettting positive score
pos.score <- vector()
length(pos.score) <- length(scores)
for (i in 1:length(scores)) {
pos = scores[[i]][2]
pos.score[i] <- pos
}
rm(i)
# Getting negative score
neg.score <- vector()
length(neg.score) <- length(scores)
for (i in 1:length(scores)) {
neg = scores[[i]][3]
neg.score[i] <- neg
}
rm(i)
neg.score
df.scores <- data.frame(tweet = txt.boulos, score = tot.score, positive.score = pos.score, negative.score = neg.score)
df.scores$score <- ifelse(df.scores$score == 0, "Neutral", ifelse(df.scores$score > 0, "Positive", "Negative"))
df.scores$score <- as.factor(df.scores$score)
df.scores %>%
group_by(score) %>%
count() %>%
ggplot(aes(x = reorder(score, -n), y = n, fill = score)) +
geom_bar(stat = "identity") +
labs(title = "Análise de Sentimento - Tweets Relacionados ao Boulos",
subtitle = "Dados coletados do twitter em 27/11/2020",
fill = "Sentimento") +
xlab(NULL) +
ylab("Quantidade de Tweets") +
theme_tinyhand()
df.scores %>%
summarise(sumtot = c(sum(positive.score), sum(negative.score))) %>%
mutate(type = c("Positive", "Negative")) %>%
ggplot(aes(x = type, y = sumtot, fill = type)) +
geom_bar(stat = "identity", show.legend = FALSE) +
labs(title = "Análise de Sentimento - Guilherme Boulos",
subtitle = "Tweets coletados em 27/11/2020",
x = "Sentimento",
y = "Quantidade") +
ggplot2::theme_minimal()
tweetcorpus <- Corpus(VectorSource(txt.boulos))
pal2 <- brewer.pal(8, "Set2")
wordcloud(tweetcorpus,
min.freq = 2,
scale = c(5, 1),
random.color = FALSE,
max.words = 60,
random.order = F,
colors = pal2)
library(wordcloud)
wordcloud(tweetcorpus,
min.freq = 2,
scale = c(5, 1),
random.color = FALSE,
max.words = 60,
random.order = F,
colors = pal2)
